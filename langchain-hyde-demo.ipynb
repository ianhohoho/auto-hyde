{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13072084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25c3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert about a set of software for building LLM-powered applications called LangChain, LangGraph, LangServe, and LangSmith.\n",
    "\n",
    "LangChain is a Python framework that provides a large set of integrations that can easily be composed to build LLM applications.\n",
    "LangGraph is a Python package built on top of LangChain that makes it easy to build stateful, multi-actor LLM applications.\n",
    "LangServe is a Python package built on top of LangChain that makes it easy to deploy a LangChain application as a REST API.\n",
    "LangSmith is a platform that makes it easy to trace and test LLM applications.\n",
    "\n",
    "Answer the user question as best you can. Answer as though you were writing a tutorial that addressed the user question.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "qa_no_context = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2547ca5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use multi-modal models in a chain and turn the chain into a REST API, you can leverage the capabilities of LangChain, LangGraph, and LangServe. Here's a step-by-step guide on how to achieve this:\n",
      "\n",
      "1. **Set up your environment**: Make sure you have Python installed on your system. You can install LangChain, LangGraph, and LangServe using pip:\n",
      "\n",
      "```bash\n",
      "pip install langchain langgraph langserve\n",
      "```\n",
      "\n",
      "2. **Create a multi-modal model**: You can create a multi-modal model using LangChain by composing different integrations. For example, you can combine text and image processing integrations to build a multi-modal model.\n",
      "\n",
      "3. **Build a stateful, multi-actor application**: Use LangGraph to build a stateful, multi-actor application that incorporates your multi-modal model. LangGraph simplifies the process of creating complex, stateful applications by providing abstractions for actors, states, and transitions.\n",
      "\n",
      "4. **Deploy the application as a REST API**: Once you have built your multi-actor application, you can use LangServe to deploy it as a REST API. LangServe makes it easy to expose your application's functionality over HTTP, allowing other services or applications to interact with it.\n",
      "\n",
      "5. **Test and trace your application**: Finally, you can use LangSmith to trace and test your LLM application. LangSmith provides tools for monitoring the behavior of your application, tracing data flow, and testing different scenarios to ensure its reliability and performance.\n",
      "\n",
      "By following these steps, you can effectively use multi-modal models in a chain and turn the chain into a REST API using LangChain, LangGraph, LangServe, and LangSmith.\n"
     ]
    }
   ],
   "source": [
    "answer = qa_no_context.invoke(\n",
    "    {\n",
    "        \"question\": \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7c8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import HypotheticalDocumentEmbedder, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab9c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_embeddings = OpenAIEmbeddings()\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "010d6924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.008367725713174623, -0.005661255873196436, -0.00837634540923301, -0.00023159501630035692, -0.003151094342014292, 0.020679192620382876, -0.034114847140683406, -0.009861742027087811, -0.012565696258124927, -0.012407280390692393]\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "multi_llm = OpenAI(n=4, best_of=4)\n",
    "embeddings = HypotheticalDocumentEmbedder.from_llm(\n",
    "    multi_llm, base_embeddings, \"web_search\"\n",
    ")\n",
    "result = embeddings.embed_query(\"Where is the Taj Mahal?\")\n",
    "print(result[:10])\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d143fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Please answer the user's question about the most recent state of the union address\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "embedding_fn = HypotheticalDocumentEmbedder(\n",
    "    llm_chain=llm_chain, base_embeddings=base_embeddings\n",
    ")\n",
    "\n",
    "result = embedding_fn.embed_query(\n",
    "    \"What did the president say about Ketanji Brown Jackson\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3309bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "with open(\"data/state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d349cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_texts(texts, embedding_fn, persist_directory=\"./data/.chroma_db\")\n",
    "\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3d5c606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-hyde",
   "language": "python",
   "name": "auto-hyde"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
