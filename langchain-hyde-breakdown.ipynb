{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85895399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46978c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hypothetical Document Embeddings.\n",
    "\n",
    "https://arxiv.org/abs/2212.10496\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "from langchain_core.callbacks import CallbackManagerForChainRun\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.pydantic_v1 import Extra\n",
    "\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains.hyde.prompts import PROMPT_MAP\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "\n",
    "class HypotheticalDocumentEmbedder(Chain, Embeddings):\n",
    "    \"\"\"Generate hypothetical document for query, and then embed that.\n",
    "\n",
    "    Based on https://arxiv.org/abs/2212.10496\n",
    "    \"\"\"\n",
    "\n",
    "    base_embeddings: Embeddings\n",
    "    llm_chain: LLMChain\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Input keys for Hyde's LLM chain.\"\"\"\n",
    "        return self.llm_chain.input_keys\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Output keys for Hyde's LLM chain.\"\"\"\n",
    "        return self.llm_chain.output_keys\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Call the base embeddings.\"\"\"\n",
    "        return self.base_embeddings.embed_documents(texts)\n",
    "\n",
    "    def combine_embeddings(self, embeddings: List[List[float]]) -> List[float]:\n",
    "        \"\"\"Combine embeddings into final embeddings.\"\"\"\n",
    "        return list(np.array(embeddings).mean(axis=0))\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate a hypothetical document and embedded it.\"\"\"\n",
    "        var_name = self.llm_chain.input_keys[0]\n",
    "        result = self.llm_chain.generate([{var_name: text}])\n",
    "        documents = [generation.text for generation in result.generations[0]]\n",
    "        for ii, doc in enumerate(documents):\n",
    "            print(f\"### Hyde Document {ii+1} ###\\n{doc}\\n\")\n",
    "        embeddings = self.embed_documents(documents)\n",
    "        return self.combine_embeddings(embeddings)\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Call the internal llm chain.\"\"\"\n",
    "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
    "        return self.llm_chain(inputs, callbacks=_run_manager.get_child())\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        base_embeddings: Embeddings,\n",
    "        prompt_key: Optional[str] = None,\n",
    "        custom_prompt: Optional[BasePromptTemplate] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> HypotheticalDocumentEmbedder:\n",
    "        \"\"\"Load and use LLMChain with either a specific prompt key or custom prompt.\"\"\"\n",
    "        if custom_prompt is not None:\n",
    "            prompt = custom_prompt\n",
    "        elif prompt_key is not None and prompt_key in PROMPT_MAP:\n",
    "            prompt = PROMPT_MAP[prompt_key]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Must specify prompt_key if custom_prompt not provided. Should be one \"\n",
    "                f\"of {list(PROMPT_MAP.keys())}.\"\n",
    "            )\n",
    "\n",
    "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        print(f\"### Hyde Propmt ###\\n{prompt}\\n\")\n",
    "        return cls(base_embeddings=base_embeddings, llm_chain=llm_chain, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"hyde_chain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a3eca6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Hyde Propmt ###\n",
      "input_variables=['QUESTION'] template='Please write a passage to answer the question \\nQuestion: {QUESTION}\\nPassage:'\n",
      "\n",
      "### Hyde Document 1 ###\n",
      " The majestic Taj Mahal is located in Agra, a city in the northern state of Uttar Pradesh in India. It is situated on the banks of the Yamuna River and is approximately 200 kilometers south of the capital city of New Delhi. The iconic monument stands as a symbol of love and is one of the most famous attractions in the world, drawing millions of visitors each year. It was built by the Mughal emperor Shah Jahan in the 17th century as a mausoleum for his beloved wife, Mumtaz Mahal. The Taj Mahal is a UNESCO World Heritage Site and is considered a masterpiece of Mughal architecture, featuring intricate marble work, beautiful gardens, and a symmetrical design. It is a must-visit destination for anyone traveling to India and is a testament to the enduring power of love and beauty. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "\n",
    "base_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "multi_llm = OpenAI(n=1, best_of=1)\n",
    "embeddings = HypotheticalDocumentEmbedder.from_llm(\n",
    "    multi_llm, base_embeddings, \"web_search\"\n",
    ")\n",
    "result = embeddings.embed_query(\"Where is the Taj Mahal?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d7a59f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "with open(\"data/state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "484c4393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Hyde Document 1 ###\n",
      " In the most recent state of the union address, President Joe Biden praised Ketanji Brown Jackson for her dedication and achievements as a judge. He also announced his nomination of her to the U.S. Court of Appeals for the District of Columbia, which if confirmed, would make her the first Black woman to serve on that court. \n",
      "\n",
      "### Hyde Document 2 ###\n",
      " During the most recent state of the union address, President Joe Biden praised the historic confirmation of Ketanji Brown Jackson as the first Black woman to serve on the U.S. Court of Appeals for the D.C. Circuit, calling her a \"trailblazing jurist\" and a \"brilliant legal mind.\" He also urged the Senate to swiftly confirm her to the Supreme Court, should a vacancy arise.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Please answer the user's question about the most recent state of the union address\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n",
    "multi_llm = OpenAI(n=2, best_of=2)\n",
    "llm_chain = LLMChain(llm=multi_llm, prompt=prompt)\n",
    "\n",
    "\n",
    "embedding_fn = HypotheticalDocumentEmbedder(\n",
    "    llm_chain=llm_chain, base_embeddings=base_embeddings\n",
    ")\n",
    "\n",
    "\n",
    "# docsearch = Chroma.from_texts(texts, embedding_fn, persist_directory=\"./data/.chroma_db\")\n",
    "\n",
    "\n",
    "# query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "# docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117050b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c58d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-hyde",
   "language": "python",
   "name": "auto-hyde"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
